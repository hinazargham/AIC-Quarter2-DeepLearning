{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reuters.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JrQy9DHGZZgg",
        "cuj893Qs9dGH"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNRxgBOBkWtW5dVEvaNN42p"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM5FfgmxwkDW"
      },
      "source": [
        "# **Classifying newswires: A multiclass classification example**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYWelAkmYhPR"
      },
      "source": [
        "\n",
        "In the previous section, you saw how to classify vector inputs into two mutually exclusive classes using a densely connected neural network. But what happens when you have more than two classes? In this section, you’ll build a network to classify Reuters newswires into 46 mutually exclusive topics. Because you have many classes, this problem is an instance of multiclass classification; and because each data point should be classified into only one category, the problem is more specifically an instance of single-label, multiclass classification. If each data point could belong to multiple categories (in this case, topics), you’d be facing a multilabel, multiclass classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L66YsC_7YlCb"
      },
      "source": [
        "## **Reuters newswires**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pso0z9g_Yudw"
      },
      "source": [
        "You’ll work with the Reuters dataset, a set of short newswires and their topics, published\n",
        "by Reuters in 1986. It’s a simple, widely used toy dataset for text classification. There\n",
        "are 46 different topics; some topics are more represented than others, but each topic\n",
        "has at least 10 examples in the training set.\n",
        "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let’s\n",
        "take a look."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrQy9DHGZZgg"
      },
      "source": [
        "### Loading Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWatKz3JZ0JV"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuj893Qs9dGH"
      },
      "source": [
        "### Loading the Reuters dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEyV22kz8_Be",
        "outputId": "85c7c605-49ab-4c84-ed69-de00bfec1339"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwIPsMMUXJM7"
      },
      "source": [
        "As with the IMDB dataset, the argument num_words=10000 restricts the data to the 10,000 most frequently occurring words found in the data.\n",
        "You have 8,982 training examples and 2,246 test examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9imvPt989ynY",
        "outputId": "b32134e2-30a3-4115-d9d3-5a359af050fc"
      },
      "source": [
        "display(len(train_data))\n",
        "display(len(train_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0NWRzdN98vf",
        "outputId": "4ff88aba-d63a-466e-8fd3-e3a61e6720c1"
      },
      "source": [
        "display(len(test_data))\n",
        "display(len(test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ2cL1jJGTXx"
      },
      "source": [
        "As with the IMDB reviews, each example is a list of integers (word indices):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYLEt7VJGHbP",
        "outputId": "3d00071b-ca4d-4da8-ee19-07a2cb3e7a1a"
      },
      "source": [
        "train_data[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 245,\n",
              " 273,\n",
              " 207,\n",
              " 156,\n",
              " 53,\n",
              " 74,\n",
              " 160,\n",
              " 26,\n",
              " 14,\n",
              " 46,\n",
              " 296,\n",
              " 26,\n",
              " 39,\n",
              " 74,\n",
              " 2979,\n",
              " 3554,\n",
              " 14,\n",
              " 46,\n",
              " 4689,\n",
              " 4329,\n",
              " 86,\n",
              " 61,\n",
              " 3499,\n",
              " 4795,\n",
              " 14,\n",
              " 61,\n",
              " 451,\n",
              " 4329,\n",
              " 17,\n",
              " 12]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqjnTY-_E00e"
      },
      "source": [
        "Here’s how you can decode it back to words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nETQS_mGa41"
      },
      "source": [
        "### Decoding newswires back to text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3uEx8G7GzoA"
      },
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n",
        "train_data[0]])\n",
        "\n",
        "# Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vks4mu64G-lT",
        "outputId": "db6a69dd-22f3-4e2c-b2ae-2fb3f7836d3b"
      },
      "source": [
        "print(decoded_newswire)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6XqqujXPdFo"
      },
      "source": [
        "The label associated with an example is an integer between 0 and 45—a topic index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWORXje0PmaD"
      },
      "source": [
        "train_data[10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvSKdhmpPcbC",
        "outputId": "969f32ec-c6c5-4587-abc0-b01475ac7678"
      },
      "source": [
        "train_labels[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSGXG8oYHJVm"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiwQ-AhmRyVZ"
      },
      "source": [
        " ### Encoding the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_ddSgb_Pyb6"
      },
      "source": [
        "import numpy as np\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.\n",
        "    return results\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSNnZT7tP8hN",
        "outputId": "ad64d048-80bb-44cd-e057-50499d0daa86"
      },
      "source": [
        "train_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]),\n",
              "       list([1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]),\n",
              "       list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 2, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12]),\n",
              "       ...,\n",
              "       list([1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, 1241, 850, 31, 56, 3890, 691, 9, 1241, 71, 9, 5985, 2, 2, 699, 2, 2, 2, 699, 244, 5945, 4, 49, 8, 4, 656, 850, 33, 2993, 9, 2139, 340, 3371, 1493, 9, 2, 22, 2, 1094, 687, 83, 35, 15, 257, 6, 57, 9190, 7, 4, 5956, 654, 5, 2, 6191, 1371, 4, 49, 8, 16, 369, 646, 6, 1076, 7, 124, 407, 17, 12]),\n",
              "       list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 3614, 18, 14, 74, 134, 5131, 18, 88, 2321, 72, 11, 14, 1842, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 3237, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 1072, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 1172, 137, 336, 1325, 6, 1532, 142, 971, 6463, 43, 359, 5, 4, 326, 753, 364, 17, 12]),\n",
              "       list([1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, 76, 7, 4, 757, 481, 3976, 790, 5259, 5654, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 1047, 91, 6917, 1688, 340, 7, 194, 9411, 6, 1894, 21, 127, 2151, 2394, 1456, 6, 3034, 4, 329, 433, 7, 65, 87, 1127, 10, 8219, 1475, 290, 9, 21, 567, 16, 1926, 24, 4, 76, 209, 30, 4033, 6655, 5654, 8, 4, 60, 8, 4, 966, 308, 40, 2575, 129, 2, 295, 277, 1071, 9, 24, 286, 2114, 234, 222, 9, 4, 906, 3994, 8519, 114, 5758, 1752, 7, 4, 113, 17, 12])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zoWjJ4OUJs-"
      },
      "source": [
        "### Change label with one-hot-encode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_m3ui09QYWD"
      },
      "source": [
        "To vectorize the labels, there are two possibilities: you can cast the label list as an integer\n",
        "tensor, or you can use one-hot encoding. One-hot encoding is a widely used format\n",
        "for categorical data, also called categorical encoding. In this case, one-hot encoding of\n",
        "the labels consists of embedding each label as an all-zero vector with a 1 in the place of\n",
        "the label index. Here’s an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zcgvppbQinq"
      },
      "source": [
        "def to_one_hot(labels, dimension=46):\n",
        "  results = np.zeros((len(labels), dimension))\n",
        "  for i, label in enumerate(labels):\n",
        "    results[i, label] = 1.\n",
        "    return results\n",
        "one_hot_train_labels = to_one_hot(train_labels) # Vectorized training labels\n",
        "one_hot_test_labels = to_one_hot(test_labels)  # Vectorized testing labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qEIJAImQ6hy"
      },
      "source": [
        "But there is a built-in way to do this in Keras, which you’ve already seen in action\n",
        "in the MNIST example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVwarsV0SPb2"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCWa1S29SYBo"
      },
      "source": [
        "### Building your network\n",
        "\n",
        "This topic-classification problem looks similar to the previous movie-review classification\n",
        "problem: in both cases, you’re trying to classify short snippets of text. But there is\n",
        "a new constraint here: the number of output classes has gone from 2 to 46. The\n",
        "dimensionality of the output space is much larger.\n",
        "In a stack of Dense layers like that you’ve been using, each layer can only access information\n",
        "present in the output of the previous layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zvnJzqWSyMx"
      },
      "source": [
        "In a stack of Dense layers like that you’ve been using, each layer can only access information\n",
        "present in the output of the previous layer. If one layer drops some information relevant to the classification problem, this information can never be recovered by later\n",
        "layers: each layer can potentially become an information bottleneck. \n",
        "\n",
        "In the previous\n",
        "example, you used 16-dimensional intermediate layers, but a 16-dimensional space may\n",
        "be too limited to learn to separate 46 different classes: such small layers may act as information\n",
        "bottlenecks, permanently dropping relevant information.\n",
        "For this reason you’ll use larger layers. \n",
        "\n",
        "Let’s go with 64 units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRNlzLIXYcdg"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrfpWjvISjVj"
      },
      "source": [
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx1qiBQVTHkH"
      },
      "source": [
        "There are two other things you should note about this architecture:\n",
        "\n",
        "1. You end the network with a Dense layer of size 46. This means for each input\n",
        "sample, the network will output a 46-dimensional vector. Each entry in this vector\n",
        "(each dimension) will encode a different output class.\n",
        "\n",
        "2. The last layer uses a softmax activation. You saw this pattern in the MNIST\n",
        "example. It means the network will output a probability distribution over the 46\n",
        "different output classes—for every input sample, the network will produce a 46-\n",
        "dimensional output vector, where output[i] is the probability that the sample\n",
        "belongs to class i. The 46 scores will sum to 1.\n",
        "The best loss function to use in this case is categorical_crossentropy. It measures\n",
        "the distance between two probability distributions: here, between the probability distribution\n",
        "output by the network and the true distribution of the labels. By minimizing\n",
        "the distance between these two distributions, you train the network to output something\n",
        "as close as possible to the true labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS8pqr6kZKcL"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hli_fM8JTNLZ"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW9rNf2WZXgs"
      },
      "source": [
        "### Validating your approach\n",
        "Let’s set apart 1,000 samples in the training data to use as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9E2EGgjTeNb"
      },
      "source": [
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Fd6DkEa0ys"
      },
      "source": [
        "### Training the model\n",
        "Now, let’s train the network for 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc33kNczTy1T",
        "outputId": "0bea106d-f481-4217-e775-17203bb3ea8b"
      },
      "source": [
        "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "16/16 [==============================] - 2s 74ms/step - loss: 3.8186 - accuracy: 0.0828 - val_loss: 3.7979 - val_accuracy: 0.0590\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 3.7928 - accuracy: 0.0788 - val_loss: 3.7770 - val_accuracy: 0.0590\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.7723 - accuracy: 0.1621 - val_loss: 3.7570 - val_accuracy: 0.0590\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 3.7522 - accuracy: 0.1318 - val_loss: 3.7372 - val_accuracy: 0.3530\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.7322 - accuracy: 0.3480 - val_loss: 3.7177 - val_accuracy: 0.3530\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.7138 - accuracy: 0.3537 - val_loss: 3.6983 - val_accuracy: 0.3530\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 3.6920 - accuracy: 0.3510 - val_loss: 3.6791 - val_accuracy: 0.3530\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 3.6734 - accuracy: 0.3507 - val_loss: 3.6600 - val_accuracy: 0.3530\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 3.6560 - accuracy: 0.3458 - val_loss: 3.6409 - val_accuracy: 0.3530\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 3.6350 - accuracy: 0.3471 - val_loss: 3.6221 - val_accuracy: 0.3530\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 3.6186 - accuracy: 0.3486 - val_loss: 3.6034 - val_accuracy: 0.3530\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.5982 - accuracy: 0.3484 - val_loss: 3.5848 - val_accuracy: 0.3530\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.5772 - accuracy: 0.3487 - val_loss: 3.5665 - val_accuracy: 0.3530\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.5629 - accuracy: 0.3458 - val_loss: 3.5482 - val_accuracy: 0.3530\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 3.5415 - accuracy: 0.3540 - val_loss: 3.5300 - val_accuracy: 0.3530\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.5251 - accuracy: 0.3467 - val_loss: 3.5120 - val_accuracy: 0.3530\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 3.5069 - accuracy: 0.3596 - val_loss: 3.4943 - val_accuracy: 0.3540\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 3.4886 - accuracy: 0.3497 - val_loss: 3.4766 - val_accuracy: 0.3540\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.4670 - accuracy: 0.3588 - val_loss: 3.4592 - val_accuracy: 0.3540\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.4515 - accuracy: 0.3515 - val_loss: 3.4418 - val_accuracy: 0.3540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prfYJNvf4nEF",
        "outputId": "e6bc0be1-0fbf-4284-b057-e04a43a8b732"
      },
      "source": [
        "history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd9e6675710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETN2WlzfT3xK",
        "outputId": "3763d48c-ea24-4299-ad4d-715e57d0b7e3"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWtjiwH-bPHc"
      },
      "source": [
        "### Plotting the training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "8wpE4_BpUAZh",
        "outputId": "e8612234-a55e-497f-fd50-cb1555f1bbda"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tp5SEH5byL8"
      },
      "source": [
        "### Plotting the training and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Yswo7tARUHzv",
        "outputId": "a978f2a6-1085-486e-bf89-216d9f0deb09"
      },
      "source": [
        "plt.clf() # clears the figure\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCVEyrPWUhI4"
      },
      "source": [
        "The network begins to overfit after nine epochs. Let’s train a new network from\n",
        "scratch for nine epochs and then evaluate it on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y67iSUqcJZW"
      },
      "source": [
        "### Retraining a model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwNka-S5Ujqn",
        "outputId": "f66a7898-0830-47db-c165-df8abeb37b48"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "partial_y_train,\n",
        "epochs=9,\n",
        "batch_size=512,\n",
        "validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 3.8185 - accuracy: 0.1242 - val_loss: 3.7980 - val_accuracy: 0.0440\n",
            "Epoch 2/9\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.7926 - accuracy: 0.1139 - val_loss: 3.7771 - val_accuracy: 0.3530\n",
            "Epoch 3/9\n",
            "16/16 [==============================] - 1s 49ms/step - loss: 3.7715 - accuracy: 0.3555 - val_loss: 3.7571 - val_accuracy: 0.3530\n",
            "Epoch 4/9\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 3.7520 - accuracy: 0.3511 - val_loss: 3.7373 - val_accuracy: 0.3530\n",
            "Epoch 5/9\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 3.7322 - accuracy: 0.3478 - val_loss: 3.7177 - val_accuracy: 0.3530\n",
            "Epoch 6/9\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 3.7121 - accuracy: 0.3560 - val_loss: 3.6984 - val_accuracy: 0.3530\n",
            "Epoch 7/9\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.6951 - accuracy: 0.3519 - val_loss: 3.6790 - val_accuracy: 0.3530\n",
            "Epoch 8/9\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 3.6728 - accuracy: 0.3644 - val_loss: 3.6599 - val_accuracy: 0.3530\n",
            "Epoch 9/9\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 3.6547 - accuracy: 0.3531 - val_loss: 3.6409 - val_accuracy: 0.3530\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 3.6433 - accuracy: 0.3615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SstXAg90U7tb",
        "outputId": "efcb661d-dab0-4db5-e24e-28d8fe326f7f"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.64326548576355, 0.3615316152572632]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW-G6ywPVEhk"
      },
      "source": [
        "This approach reaches an accuracy of ~80%. With a balanced binary classification\n",
        "problem, the accuracy reached by a purely random classifier would be 50%. But in\n",
        "this case it’s closer to 19%, so the results seem pretty good, at least when compared to\n",
        "a random baseline:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFgTvkJcVQdD"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        ">>> import copy\n",
        ">>> test_labels_copy = copy.copy(test_labels)\n",
        ">>> np.random.shuffle(test_labels_copy)\n",
        ">>> hits_array = np.array(test_labels) == np.array(test_labels_copy)\n",
        ">>> float(np.sum(hits_array)) / len(test_labels)\n",
        "0.18655387355298308\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RY6f1D3dJuQ"
      },
      "source": [
        "### Generating predictions on new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10iWbsA3VdMR"
      },
      "source": [
        "You can verify that the predict method of the model instance returns a probability\n",
        "distribution over all 46 topics. Let’s generate topic predictions for all of the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKCfAW1cVnve"
      },
      "source": [
        "#### Generating predictions for new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlmRNpSfVh2a",
        "outputId": "7717df68-2f69-4900-b331-dcbfacdf7c65"
      },
      "source": [
        "predictions = model.predict(x_test)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02348575, 0.02698697, 0.02099276, ..., 0.02173574, 0.02330261,\n",
              "        0.01788807],\n",
              "       [0.02067538, 0.02761782, 0.02069566, ..., 0.02059569, 0.020576  ,\n",
              "        0.02057744],\n",
              "       [0.02067538, 0.02761782, 0.02069566, ..., 0.02059569, 0.020576  ,\n",
              "        0.02057744],\n",
              "       ...,\n",
              "       [0.02067538, 0.02761782, 0.02069566, ..., 0.02059569, 0.020576  ,\n",
              "        0.02057744],\n",
              "       [0.02067538, 0.02761782, 0.02069566, ..., 0.02059569, 0.020576  ,\n",
              "        0.02057744],\n",
              "       [0.02067538, 0.02761782, 0.02069566, ..., 0.02059569, 0.020576  ,\n",
              "        0.02057744]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j3TTIDPV0Xb"
      },
      "source": [
        "Each entry in predictions is a vector of length 46:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqVui432Vt-u",
        "outputId": "d76c44eb-c54b-4dd5-ed9d-f7429ac27403"
      },
      "source": [
        "predictions[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PomccldV3rD"
      },
      "source": [
        "The coefficients in this vector sum to 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1xT3UTQV7rl",
        "outputId": "8b19f9b3-37e6-40e3-ac18-6dc9747386f4"
      },
      "source": [
        "np.sum(predictions[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99999994"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e11xD22CWEfm"
      },
      "source": [
        "The largest entry is the predicted class—the class with the highest probability:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh-5RnoHWDFD",
        "outputId": "8548b086-7244-47b9-beff-128346d74aa5"
      },
      "source": [
        "np.argmax(predictions[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FTn8rKQ9ttl"
      },
      "source": [
        "## A different way to handle the labels and the loss\n",
        "\n",
        "We mentioned earlier that another way to encode the labels would be to cast them as\n",
        "an integer tensor, like this:\n",
        "\n",
        "```\n",
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)\n",
        "```\n",
        "\n",
        "The only thing this approach would change is the choice of the loss function. The loss\n",
        "function categorical_crossentropy, expects the labels to follow\n",
        "a categorical encoding. With integer labels, you should use sparse_categorical_\n",
        "crossentropy:\n",
        "\n",
        "```\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='sparse_categorical_crossentropy',\n",
        "metrics=['acc'])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic7JA6WS-iTE"
      },
      "source": [
        "This new loss function is still mathematically the same as categorical_crossentropy;\n",
        "it just has a different interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPeNRvTG-uZz"
      },
      "source": [
        "## The importance of having sufficiently large intermediate layers\n",
        "We mentioned earlier that because the final outputs are 46-dimensional, you should\n",
        "avoid intermediate layers with many fewer than 46 hidden units. Now let’s see what\n",
        "happens when you introduce an information bottleneck by having intermediate layers\n",
        "that are significantly less than 46-dimensional: for example, 4-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_twHlOL-d-J",
        "outputId": "da0bac85-1462-46dd-cb93-289229927ebc"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(4, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "partial_y_train,\n",
        "epochs=20,\n",
        "batch_size=128,\n",
        "validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 2s 23ms/step - loss: 3.8015 - accuracy: 0.2571 - val_loss: 3.7416 - val_accuracy: 0.3540\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.7231 - accuracy: 0.3421 - val_loss: 3.6680 - val_accuracy: 0.3540\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.6481 - accuracy: 0.3539 - val_loss: 3.5963 - val_accuracy: 0.3540\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.5798 - accuracy: 0.3491 - val_loss: 3.5267 - val_accuracy: 0.3540\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.5106 - accuracy: 0.3571 - val_loss: 3.4593 - val_accuracy: 0.3540\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.4431 - accuracy: 0.3585 - val_loss: 3.3940 - val_accuracy: 0.3540\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.3801 - accuracy: 0.3539 - val_loss: 3.3307 - val_accuracy: 0.3540\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.3126 - accuracy: 0.3476 - val_loss: 3.2697 - val_accuracy: 0.3540\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.2561 - accuracy: 0.3536 - val_loss: 3.2111 - val_accuracy: 0.3540\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.1994 - accuracy: 0.3561 - val_loss: 3.1542 - val_accuracy: 0.3540\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.1383 - accuracy: 0.3473 - val_loss: 3.1000 - val_accuracy: 0.3540\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.0948 - accuracy: 0.3502 - val_loss: 3.0472 - val_accuracy: 0.3540\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.0403 - accuracy: 0.3490 - val_loss: 2.9965 - val_accuracy: 0.3540\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.9762 - accuracy: 0.3562 - val_loss: 2.9483 - val_accuracy: 0.3540\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 2.9437 - accuracy: 0.3541 - val_loss: 2.9017 - val_accuracy: 0.3540\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.8975 - accuracy: 0.3477 - val_loss: 2.8570 - val_accuracy: 0.3540\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.8432 - accuracy: 0.3539 - val_loss: 2.8148 - val_accuracy: 0.3540\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.8191 - accuracy: 0.3448 - val_loss: 2.7743 - val_accuracy: 0.3540\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.7655 - accuracy: 0.3540 - val_loss: 2.7361 - val_accuracy: 0.3540\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.7291 - accuracy: 0.3543 - val_loss: 2.7004 - val_accuracy: 0.3540\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd9f2202a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuD_xKAJ-8h1"
      },
      "source": [
        "The network now peaks at ~71% validation accuracy, an 8% absolute drop. This drop\n",
        "is mostly due to the fact that you’re trying to compress a lot of information (enough\n",
        "information to recover the separation hyperplanes of 46 classes) into an intermediate\n",
        "space that is too low-dimensional. The network is able to cram most of the necessary\n",
        "information into these eight-dimensional representations, but not all of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8nNGsbsFAzA"
      },
      "source": [
        "## Further experiments\n",
        "1. Try using larger or smaller layers: 32 units, 128 units, and so on.\n",
        "\n",
        "2. You used two hidden layers. Now try using a single hidden layer, or three hidden\n",
        "layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tURfpJqEExTx",
        "outputId": "4e82d62e-3a89-4f1b-d383-510a845d3b80"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "partial_y_train,\n",
        "epochs=20,\n",
        "batch_size=128,\n",
        "validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 2s 21ms/step - loss: 3.8016 - accuracy: 0.1776 - val_loss: 3.7418 - val_accuracy: 0.2220\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.7240 - accuracy: 0.2236 - val_loss: 3.6679 - val_accuracy: 0.2220\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.6501 - accuracy: 0.2882 - val_loss: 3.5964 - val_accuracy: 0.3530\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.5783 - accuracy: 0.3564 - val_loss: 3.5270 - val_accuracy: 0.3540\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.5109 - accuracy: 0.3449 - val_loss: 3.4592 - val_accuracy: 0.3540\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.4375 - accuracy: 0.3502 - val_loss: 3.3941 - val_accuracy: 0.3540\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.3768 - accuracy: 0.3576 - val_loss: 3.3307 - val_accuracy: 0.3540\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.3196 - accuracy: 0.3506 - val_loss: 3.2698 - val_accuracy: 0.3540\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.2530 - accuracy: 0.3554 - val_loss: 3.2112 - val_accuracy: 0.3540\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.1942 - accuracy: 0.3551 - val_loss: 3.1544 - val_accuracy: 0.3540\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.1441 - accuracy: 0.3514 - val_loss: 3.1000 - val_accuracy: 0.3540\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.0816 - accuracy: 0.3631 - val_loss: 3.0476 - val_accuracy: 0.3540\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.0372 - accuracy: 0.3514 - val_loss: 2.9968 - val_accuracy: 0.3540\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.9940 - accuracy: 0.3478 - val_loss: 2.9483 - val_accuracy: 0.3540\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.9488 - accuracy: 0.3526 - val_loss: 2.9019 - val_accuracy: 0.3540\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.9039 - accuracy: 0.3550 - val_loss: 2.8573 - val_accuracy: 0.3540\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.8437 - accuracy: 0.3535 - val_loss: 2.8147 - val_accuracy: 0.3540\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.8029 - accuracy: 0.3554 - val_loss: 2.7747 - val_accuracy: 0.3540\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.7627 - accuracy: 0.3571 - val_loss: 2.7364 - val_accuracy: 0.3540\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 2.7276 - accuracy: 0.3532 - val_loss: 2.7006 - val_accuracy: 0.3540\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd9f24b22d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXrJg1SNFgGe",
        "outputId": "819c830b-d540-434e-f086-6fb6c469f330"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "partial_y_train,\n",
        "epochs=20,\n",
        "batch_size=128,\n",
        "validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 2s 22ms/step - loss: 3.8015 - accuracy: 0.1899 - val_loss: 3.7416 - val_accuracy: 0.2220\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.7224 - accuracy: 0.2719 - val_loss: 3.6679 - val_accuracy: 0.2220\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.6508 - accuracy: 0.2582 - val_loss: 3.5961 - val_accuracy: 0.2230\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 3.5776 - accuracy: 0.2939 - val_loss: 3.5263 - val_accuracy: 0.3540\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 3.5114 - accuracy: 0.3519 - val_loss: 3.4588 - val_accuracy: 0.3540\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.4411 - accuracy: 0.3569 - val_loss: 3.3935 - val_accuracy: 0.3540\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.3769 - accuracy: 0.3523 - val_loss: 3.3305 - val_accuracy: 0.3540\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.3138 - accuracy: 0.3490 - val_loss: 3.2698 - val_accuracy: 0.3540\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.2583 - accuracy: 0.3405 - val_loss: 3.2112 - val_accuracy: 0.3540\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.1943 - accuracy: 0.3565 - val_loss: 3.1545 - val_accuracy: 0.3540\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.1438 - accuracy: 0.3495 - val_loss: 3.0997 - val_accuracy: 0.3540\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.0851 - accuracy: 0.3473 - val_loss: 3.0474 - val_accuracy: 0.3540\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.0280 - accuracy: 0.3625 - val_loss: 2.9968 - val_accuracy: 0.3540\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.9917 - accuracy: 0.3543 - val_loss: 2.9482 - val_accuracy: 0.3540\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.9437 - accuracy: 0.3478 - val_loss: 2.9013 - val_accuracy: 0.3540\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 2.8911 - accuracy: 0.3528 - val_loss: 2.8571 - val_accuracy: 0.3540\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.8483 - accuracy: 0.3535 - val_loss: 2.8146 - val_accuracy: 0.3540\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 2.8155 - accuracy: 0.3495 - val_loss: 2.7740 - val_accuracy: 0.3540\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 2.7714 - accuracy: 0.3504 - val_loss: 2.7358 - val_accuracy: 0.3540\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 2.7487 - accuracy: 0.3466 - val_loss: 2.6998 - val_accuracy: 0.3540\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd9e5674a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oHKYr82EvaX",
        "outputId": "c40c5bd5-df22-4d98-eded-64c56dcc6b62"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(4, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "partial_y_train,\n",
        "epochs=20,\n",
        "batch_size=128,\n",
        "validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 2s 21ms/step - loss: 3.8016 - accuracy: 0.2249 - val_loss: 3.7421 - val_accuracy: 0.2220\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.7224 - accuracy: 0.3302 - val_loss: 3.6684 - val_accuracy: 0.3530\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.6500 - accuracy: 0.3523 - val_loss: 3.5966 - val_accuracy: 0.3530\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.5788 - accuracy: 0.3624 - val_loss: 3.5270 - val_accuracy: 0.3530\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.5107 - accuracy: 0.3506 - val_loss: 3.4595 - val_accuracy: 0.3530\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.4461 - accuracy: 0.3517 - val_loss: 3.3940 - val_accuracy: 0.3530\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.3791 - accuracy: 0.3479 - val_loss: 3.3310 - val_accuracy: 0.3530\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.3197 - accuracy: 0.3482 - val_loss: 3.2699 - val_accuracy: 0.3530\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.2553 - accuracy: 0.3546 - val_loss: 3.2109 - val_accuracy: 0.3530\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.2026 - accuracy: 0.3401 - val_loss: 3.1545 - val_accuracy: 0.3530\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.1398 - accuracy: 0.3532 - val_loss: 3.0996 - val_accuracy: 0.3530\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.0859 - accuracy: 0.3487 - val_loss: 3.0470 - val_accuracy: 0.3530\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 3.0323 - accuracy: 0.3443 - val_loss: 2.9963 - val_accuracy: 0.3530\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.9782 - accuracy: 0.3628 - val_loss: 2.9478 - val_accuracy: 0.3530\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.9349 - accuracy: 0.3522 - val_loss: 2.9013 - val_accuracy: 0.3530\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.8890 - accuracy: 0.3517 - val_loss: 2.8566 - val_accuracy: 0.3530\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.8557 - accuracy: 0.3478 - val_loss: 2.8143 - val_accuracy: 0.3530\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.7990 - accuracy: 0.3583 - val_loss: 2.7742 - val_accuracy: 0.3530\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.7620 - accuracy: 0.3556 - val_loss: 2.7360 - val_accuracy: 0.3530\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 2.7304 - accuracy: 0.3524 - val_loss: 2.7001 - val_accuracy: 0.3530\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd9e4478ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t18Wafq_KHO"
      },
      "source": [
        "## Wrapping up\n",
        "\n",
        "If you’re trying to classify data points among N classes, your network should end\n",
        "with a Dense layer of size N. In a single-label, multiclass classification problem, your network should end\n",
        "with a softmax activation so that it will output a probability distribution over the\n",
        "N output classes.\n",
        "\n",
        "1.Categorical crossentropy is almost always the loss function you should use for\n",
        "such problems. It minimizes the distance between the probability distributions\n",
        "output by the network and the true distribution of the targets.\n",
        "\n",
        "2.There are two ways to handle labels in multiclass classification:\n",
        "\n",
        "\n",
        "*   Encoding the labels via categorical encoding (also known as one-hot encoding and using categorical_crossentropy as a loss function\n",
        "*   Encoding the labels as integers and using the sparse_categorical_crossentropy\n",
        "loss function\n",
        "\n",
        "3.If you need to classify data into a large number of categories, you should avoid\n",
        "creating information bottlenecks in your network due to intermediate layers\n",
        "that are too small."
      ]
    }
  ]
}